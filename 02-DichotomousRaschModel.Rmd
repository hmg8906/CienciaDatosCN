# Dichotomous Rasch Models {#Dich_Rasch_model}

## Dichotomous Rasch Model

### Definition

$$\ln_{}{}\left[\frac{\phi_{n i 1}}{\phi_{n i 0}}\right]=\theta_{n}-\delta_{i}$$
The Rasch model predicts the probability of person *n* on item *i* providing a correct (x = 1), rather than incorrect (x = 0) response, given a person’s “ability” (*θn*) and an item’s difficulty (*δi*).

### Basic Rasch Model & IRT Model Assumptions

- Each person can be characterized by an ability (i.e., level), denoted as *θn*

- Each item can be characterized by a difficulty, denoted as *δi*

- Person ability and item difficulty can be expressed on one linear continuum (line)

- The probability of observing any particular response can be calculated from the difference between ability and difficulty

## R-Lab: Running Dichotomous Rasch Model in R
### Understand the Data set
In this example,we will be working with data from a transitive reasoning test, which is a reasoning test related to relationships among physical objects. The transitive reasoning data were collected from a one-on-one interactive assessment in which an experimenter presented students with a set of objects, such as sticks, balls, cubes, and discs. The following description is given in [Sijtsma and Molenaar](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) (2002), pp. 31-32:

> The items for transitive reasoning had the following structure. A typical item used three sticks, here denoted A, B, and C, of different length, denoted Y, such that YA < YB < YC. The actual test taking had the form of a conversation between experimenter and child in which the sticks were identified by their colors rather than letters. First, sticks A and B were presented to a child, who was allowed to pick them up and compare their lengths, for example, by placing them next to each other on a table. 

> Next, sticks B and C were presented and compared. Then all three sticks were displayed in a random order at large mutual distances so that their length differences were imperceptible, and the child was asked to infer the relation between sticks A and C from his or her knowledge of the relationship in the other two pairs.

The transitive reasoning items varied in terms of the property students were asked to reason about (length, weight, area). The tasks also varied in terms of the number of items students were asked to reason about, and whether the tasks involved equalities, inequalities, or a mixture of equalities and inequalities. The characteristics of the transitive reasoning data are summarized in the following table:

|Task|Property|Format|Objects|Measures|
| :--- | :---- |  :---- | :---- | :---- |
|1|Length|YA > YB > YC|Sticks|12, 11.5, 11 (cm)|
|2|Length|YA = YB = YC = YD |Tubes|12 (cm)|
|3|Weight|YA > YB > YC|Tubes|45, 25, 18 (g)|
|4|Weight|YA = YB = YC = YD|Cubes|65 (g)|
|5|Weight|YA < YB < YC|Balls|40, 50, 70 (g)|
|6|Area|YA > YB> YC|Discs|2.5, 7, 6.5 (diameter; cm)|
|7|Length|YA > YB = YC|Sticks|28.5, 27.5, 27.5 (cm)|
|8|Weight|YA >YB = YC|Balls|65, 40, 40 (g)|
|9|Length|YA = YB = YC = YD|Sticks|12.5, 12.5, 13, 13 (cm)|
|10|Weight|YA = YB < YC = YD|Balls|60, 60, 100, 100 (g)|


### Prepare the R Packages for analysis
In this session, we use "TAM" package to do the Rasch Analysis. TAM is actually the abbreviation for *"Test Analysis Modules"*. Different from the *Winstep* software, it applies marginal maximum likelihood estimation (MMLE) instead of joint maximum likelihood estimation (JMLE).
Note: Because of the different way of the estimation, the result might be slightly different with your *Winstep* outcomes.

```{r message=FALSE, warning=FALSE}
# Load the R package that we need for this analysis
library("TAM") # For Dichotomous Rasch Analysis
library("WrightMap") # For plotting the variable map
library("Hmisc") # For descriptive data analysis
library("formattable") # For format number as percentage
```

### Import the data & Running descriptive analysis
```{r message=FALSE}
library(readr) # Import the data from your computer
transreas <- read_csv("transreas.csv")
```
Then,we run a descriptive analysis on our data.
```{r}
# Use the summary() function to overview the data
summary(transreas)
# From the result, we can see there is no missing data for each variable. And we can also get a general idea on the range of the grades (from 2 to 6), Min, Max, Median for each task. 
# You can also use describe() function as an alternative approach
describe(transreas)
# The Info in this table is exactly the Proportion Correct
```

### Runing the Dichotomous Rasch Model
For running the dichotomous Rasch Model using TAM package, we only need students' scores. So the first and second column of our data are not needed. 
```{r}
# Trim the data
Di_Rash_data <- transreas[,c(-1,-2)]
head(Di_Rash_data) # Take a look
```

```{r results='hide'}
# Running the Rasch Dichotomous Model
Di_Rasch_model <- tam(Di_Rash_data)
```

### Overall Model Summary

```{r}
# Check the summary
summary(Di_Rasch_model)
# Plot the variable-Map
IRT.WrightMap(Di_Rasch_model,show.thr.lab=FALSE)
```

### Item Parameters
First of all, let's pool out the item parameters from your model.
```{r}
difficulty <- Di_Rasch_model$xsi
head(difficulty) 
mean(difficulty$xsi) # The mean difficulty of the Tasks is -1.936
sd(difficulty$xsi) # The standard deviation for task difficulty is 1.56
mean(difficulty$se.xsi) # The Standard Error for task is 0.171.
```

- The 'xis' column denotes the item difficulty in a logit scale. And 'se.xsi' is the standard error for each item. The standard error tells us whether item difficulties may overlap or not.

- Since the `xsi` indicates the item difficulty, the higher the value refer to the harder the item. For instance, item 9 is the hardest item ( `xsi` = 1.00), whereas the item 6 is the easiest item (`xsi` = -4.07).

We can also visualize the item difficulty by using a simple histogram
```{r}
hist(difficulty$xsi,breaks=10) 
```

Now let's calculate the Item fit statistics
```{r}
Di_Item_fit <- tam.fit(Di_Rasch_model) 
head(Di_Item_fit)
```

### Person Parameters
Get the person ability by using `tam.wle` function

```{r}
Person_ability <- tam.wle(Di_Rasch_model)
# View(Person_ability)
# In the data frame above, the `theta` is the person's ability measure in a logit scale.
mean(Person_ability$theta) # The average ability for task taker is -0.056.
sd(Person_ability$theta) # The standard deviation for task taker is 1.28.
mean(Person_ability$error) # The Standard Error for task taker is 1.023.
``` 

Visualize the Person ability by using a simple histogram
```{r}
hist(Person_ability$theta)
```

Calculate the Person fit statistics
```{r}
Di_Person_fit <- tam.personfit(Di_Rasch_model) 
head(Di_Person_fit)
```

### Prepare the Data Result table
> For Item Calibration Table

```{r}
# Set up the contents for table2
Table2 <- data.frame()
Table2 <- setNames(data.frame(matrix(ncol = 8, nrow = 10)), c("TaskID", "PropCorrect", "Delta","SE","Outfit","Outfit_P","Infit","Infit_P"))
# Calculate the proportion correct (you can also type in these values by hand from the previous outcome)
TaskCorrect <- apply(Di_Rash_data, 2, sum)
PropCorrect <- percent(TaskCorrect/425)
Table2$TaskID <- 1:10
Table2$PropCorrect <- PropCorrect
Table2$Delta <- difficulty$xsi
Table2$SE <- difficulty$se.xsi
Table2$Outfit <- Di_Item_fit[["itemfit"]][["Outfit"]]
Table2$Outfit_P <- Di_Item_fit[["itemfit"]][["Outfit_p"]]
Table2$Infit <- Di_Item_fit[["itemfit"]][["Infit"]]
Table2$Infit_P <- Di_Item_fit[["itemfit"]][["Infit_p"]]
# Sort the table 2 by Item difficulty
Table2 <- Table2[order(-PropCorrect),]
```

> For Person Calibration Table

```{r}
# Set up the contents for table3
Table3 <- data.frame()
Table3 <- setNames(data.frame(matrix(ncol = 8, nrow = 425)), c("TestTakerID", "PropCorrect", "Theta","SE","Outfit","Outfit_t","Infit","Infit_t"))
# Calculate the Porp Correct
Person_Score <- rowSums(Di_Rash_data, na.rm=FALSE) 
Person_PropCorrect <- Person_Score/10
Table3$TestTakerID <- 1:425
Table3$PropCorrect <- Person_PropCorrect
Table3$Theta <- Person_ability$theta
Table3$SE <- Person_ability$error
Table3$Outfit <- Di_Person_fit$outfitPerson
Table3$Outfit_t <- Di_Person_fit$outfitPerson_t
Table3$Infit <- Di_Person_fit$infitPerson
Table3$Infit_t <- Di_Person_fit$infitPerson_t 
# Note here the TAM package only report t value instead of p value. However, you still can calculate that by yourself if you need it.
```



## Example APA-Style Results Write-Up (Transitive Reasoing Test)

Table 1 presents a summary of the results from the analysis of the transitive reasoning data [Sijtsma and Molenaar,2002](https://methods.sagepub.com/book/introduction-to-nonparametric-item-response-theory) using the dichotomous Rasch model ([Rasch, 1960](https://eric.ed.gov/?id=ED419814)). Specifically, the calibration of test participants (*N* = 425) and Tasks (*N* = 10) are summarized using average logit-scale calibrations, standard errors, and model-data fit statistics. Examination of the results indicates that, on average, the task takers were located higher on the logit scale (*M* = -0.056,*SD* = 1.281), compared to Tasks (*M* = -1.936, *SD* = 1.281). This finding suggests that the items were relatively easy for the sample of kids who participated in this transitive reasoning test. However, average values of the Standard Error (*SE*) are slightly higher for Kids (*M* = 1.023) than Tasks (*M* = 0.17), indicating that there may be some issues related to targeting for some of the Kids who participated in the assessment. Average values of model-data fit statistics indicate overall adequate fit to the model, with average Infit and Outfit mean square statistics around 1.00, [and average standardized Infit and Outfit statistics near the expected value of 0.00 when data fit the model.] **This sentence needs rephrase.** This finding of adequate fit to the model supports the interpretation of item and person calibrations on the logit scale as indicators of their locations on the latent variable measured by the test.

```{r}
# Print the table2 in a neat way
knitr::kable(
  Table2[,-1], booktabs = TRUE,
  caption = 'Item Calibration'
)
```

Table 2.1 includes detailed results for the 10 Task items included in the Transitive Reasoning test. For each item, the proportion of correct responses is presented, followed by the logit-scale calibration (*δ*), SE, and model-data fit statistics. Examination of these results indicates that Task 9 was the most difficult (*Proportion Correct*  = 30.12%; *δ* = 1.00 ; *SE*  = .11), followed by Task 10 (Proportion Correct  = 52%; *δ* = -.09; *SE*  = 0.11). The easiest item was Task 6(*Proportion Correct* = 97.41%; *δ* = -4.07; *SE* = 0.31).

```{r}
# Print the table3 in a neat way
knitr::kable(
  head(Table3,10), booktabs = TRUE,
  caption = 'Person Calibration'
)
```

Table 3 includes detailed results for first 10 test takers who participated in the Transitive Reasoning Test. For each participant, the proportion of correct responses is presented, followed by their logit-scale measure (*θ*), *SE*, and model-data fit statistics. Examination of these results indicates that around 51 participants has the highest score (*Proportion Correct* = 100%; *θ* = 2.347; *SE* = 1.762). The lowest score test taker was *ID.148* (*Proportion Correct* = 10%; *θ* = -4.52; *SE* = 1.03).

```{r}
# Plot the variable-Map
IRT.WrightMap(Di_Rasch_model,show.thr.lab=FALSE)
```

Figure 1 illustrates the calibrations of the Participants and Items on the logit scale that represents the latent variable. The calibrations shown in this figure correspond to the calibrations presented in Table 2 and Table 3 for items and persons, respectively. The rightmost column (Measure) shows the logit scale. Higher numbers correspond to higher levels of achievement (for persons) and higher levels of difficulty (for items), and lower numbers correspond to lower achievement and less difficulty, respectively, for persons and items. Next, Respondents on the latent variable are illustrated using the histogram. Examination of the histogram indicates a wide spread of achievement levels, with most students grouped near the middle of the logit scale (*θ* = 0.00). Next, Task locations on the logit scale are plotted on the right side. Examination of the Tasks plotting indicates a similar overall spread as the participants measures. However, the Tasks appear somewhat clustered at the lower half of the logit scale, without many items appearing above average (*θ* >= 0.00). This lack of moderate-difficulty items may have contributed to the somewhat large SE values for students with middle-range calibrations.  

## Exericise

Use the Simulated Data to Run Dichotomous Rasch Model using TAM package.
[The Data could be either attached to this site or Blackboard]

## Supplementary Learning Materials

[Rasch Estimation Demonstration Spreadsheet](https://www.rasch.org/moulton.xls)

[Li, Y. Using the open-source statistical language R to analyze the dichotomous Rasch model. Behavior Research Methods 38, 532–541 (2006). https://doi.org/10.3758/BF03192809](https://link.springer.com/content/pdf/10.3758/BF03192809.pdf)

[Rasch, G. (1960/1980). Probabilistic models for some intelligence and attainment tests.(Copenhagen, Danish Institute for Educational Research), expanded edition (1980) with foreword and afterword by B.D. Wright. Chicago: The University of Chicago Press.](https://eric.ed.gov/?id=ED419814)

[Wright, B. D., & Masters, G. N. (1982). Rating Scale Analysis: Rasch Measurement. Chicago, IL: MESA Press.](https://pdfs.semanticscholar.org/8083/5035228bc338840ed6c67e879b4bcef11e07.pdf?_ga=2.216101590.1797749273.1596845271-1703835138.1596845271)


